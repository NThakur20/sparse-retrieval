from typing import List

from numpy import require
from . import encode
from . import quantize
from . import index
from . import reformat_query
from . import search
from . import evaluate
import argparse
import os


def run(
    # encode
    encoder_name: str,
    ckpt_name: str,
    data_name: str,
    data_dir: str,
    gpus: List[int],
    output_dir: str,

    # quantize
    do_quantization: bool,
    quantization_method: str,
    original_score_range: float,
    quantization_nbits: int,
    ndigits: int,

    # reformat_query
    original_query_format: str,

    # search
    topic_split: str = 'test',
    hits: int = 1000,
    output_format_search: str = 'trec',

    # evaluate
    k_values: List[int] = [1, 3, 5, 10, 100, 1000],

    # default setting
    batch_size: int = 64,
    chunk_size: int = 100000,
    nprocs: int = 12,
):
    # 1. Encode the documents into term weights
    # The output will be ${output_dir}/collection
    output_dir_encode = os.path.join(output_dir, 'collection')
    if not os.path.exists(output_dir_encode):
        encode.run(encoder_name, ckpt_name, data_name, data_dir, gpus, output_dir_encode, batch_size, chunk_size)
    else:
        print('Escaped encoding due to the existing output file(s)')

    # 2. Quantize the term weights from floats into integers
    # The output will be ${output_dir}-quantized/collection, if `do_quantization == True`
    collection_dir = os.path.join(output_dir, 'collection')
    output_dir_quantize = None
    if do_quantization:
        output_dir += '-quantized'  # TODO: Change this into a specific name
        output_dir_quantize = os.path.join(output_dir, 'collection')
    if not os.path.exists(output_dir_quantize):
        quantize.run(collection_dir, output_dir_quantize, quantization_method, original_score_range, quantization_nbits, ndigits, not do_quantization, nprocs)
    else:
        print('Escaped quantization due to the existing output file(s)')
    if do_quantization:
        collection_dir = output_dir_quantize

    # 3. Index the term weights into a Lucene-format index file
    # The output will be ${output_dir}-quantized/index
    output_dir_index = os.path.join(output_dir, 'index')
    if not os.path.exists(output_dir_index):
        index.run('JsonVectorCollection', collection_dir, output_dir_index, 'DefaultLuceneDocumentGenerator', True, True, nprocs)
    else:
        print('Escaped indexing due to the existing output file(s)')

    # 4. Reformat the queries into Pyserini-compatible
    # The output will be under the data directory by default
    if ('beir/' in data_name or 'beir_' in data_name) and data_dir is None:  # TODO: Unify this along with the same snippet within data_iters.py
        data_dir = os.path.join('datasets', data_name.replace('beir_', 'beir/'))
    reformatted_queries_path = os.path.join(data_dir, f'queries-{topic_split}.reformatted.tsv')
    if not os.path.exists(reformatted_queries_path):
        reformat_query.run(original_query_format, data_dir, None)
    else:
        print('Escaped reformatting quries due to the existing output file(s)')

    # 5. Search the queries over the index
    # The output will be ${output_dir}-quantized/${output_format_search}-format/run.tsv
    output_path_search = os.path.join(output_dir, f'{output_format_search}-format/run.tsv')
    if not os.path.exists(output_path_search):
        search.run(
            topics=reformatted_queries_path,
            encoder=ckpt_name,
            index=output_dir_index,
            output=output_path_search,
            impact=True,
            hits=hits,
            batch_size=batch_size,
            threads=nprocs,
            output_format=output_format_search
        )
    else:
        print('Escaped search due to the existing output file(s)')

    # 6. Do evaluation over the results generated by the last step
    # The output will be under ${output_dir}-quantized/evaluation
    qrels_path = os.path.join(data_dir, 'qrels', f'{topic_split}.tsv')
    output_dir_evaluate = os.path.join(output_dir, 'evaluation')
    if not os.path.exists(output_dir_evaluate):
        evaluate.run(output_path_search, output_format_search, qrels_path, output_dir_evaluate, k_values)
    else:
        print('Escaped evaluation due to the existing output file(s)')

    print('Done!')

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--encoder_name')
    parser.add_argument('--ckpt_name')
    parser.add_argument('--data_name')
    parser.add_argument('--data_dir', required=False)
    parser.add_argument('--gpus', nargs='+', type=int)
    parser.add_argument('--output_dir')

    parser.add_argument('--do_quantization', action='store_true')
    parser.add_argument('--quantization_method', required=False, choices=['range-nbits', 'ndigits-round'])
    # for 'range-nbits':
    parser.add_argument('--original_score_range', type=float, default=5)
    parser.add_argument('--quantization_nbits', type=int, default=8)
    # for 'ndigits-round':
    parser.add_argument('--ndigits', type=int, default=2, help='2 means *100')

    parser.add_argument('--original_query_format', help='e.g. beir')

    parser.add_argument('--topic_split', choices=['train', 'test', 'dev'], help='The queries split for search and evaluation')
    parser.add_argument('--hits', type=int, default=1000)
    parser.add_argument('--output_format_search', type=str, default='trec', choices=['msmarco', 'trec'])

    parser.add_argument('--k_values', nargs='+', type=int, default=[1,3,5,10,100,1000])

    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--chunk_size', type=int, default=100000)
    parser.add_argument('--nprocs', type=int, default=12)

    args = parser.parse_args()
    run(**vars(args))

